# ğŸ“Š DataForge

**DataForge** is a Python tool that helps teams quickly generate usable datasets using AI. It fetches real data from Kaggle, creates fake but realistic versions using an LLM, and makes them securely accessible via GUI and API. Perfect for safe prototyping and model testing.

---

## ğŸš€ Features

- ğŸ” **Keyword-based Search**: Input a topic (e.g., "housing") and automatically fetch related datasets from Kaggle using the Kaggle API.
- ğŸ“¥ **Reference Dataset Extraction**: Automatically downloads and parses relevant Kaggle datasets.
- ğŸ¤– **Synthetic Dataset Generation**: Uses Mistral (via Ollama, running locally) to generate multiple, non-identical datasets based on the original schema.
- ğŸ—‚ï¸ **Organized Storage**: Datasets are grouped into folders:
  - `/kaggle_datasets/` â€“ original reference data
  - `/generated_datasets/` â€“ AI-generated data
- ğŸ–¥ï¸ **User-Friendly GUI**: Built with CustomTkinter for smooth input, progress tracking, and dataset previews.

---

## ğŸ–¼ï¸ GUI Preview

> <img width="1051" alt="{28538D5B-E1CF-499B-B2E3-C66A00EEF1ED}" src="https://github.com/user-attachments/assets/d4e775cb-1f64-473e-814d-f55085751b4c" />

---

## ğŸ”§ How It Works

1. Launch the GUI.
2. Enter a keyword (e.g., `weather`, `finance`, `real estate`).
3. DataForge:
   - Uses your Kaggle API key to find the most relevant dataset.
   - Downloads and extracts the data.
   - Sends a schema-based prompt to Mistral running locally in Ollama.
   - Generates 5â€“6 new datasets with varied but related content.
4. Outputs are saved locally and can be previewed or reused.

---

## ğŸ“ Folder Structure

```plaintext
DataForge/
â”œâ”€â”€ main.py                          # Application entry point
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ config.json                      # Configuration file
â”œâ”€â”€ setup.py                         # Installation script
â”œâ”€â”€ README.md                        # Documentation
â”œâ”€â”€ LICENSE                          # License file
â”œâ”€â”€ .gitignore                       # Git ignore file
â”œâ”€â”€ src/
â”‚   â””â”€â”€ dataforge/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ config_manager.py    # Configuration management
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ dataset_generator.py # Core generation logic
â”‚       â”œâ”€â”€ handlers/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ kaggle_handler.py    # Kaggle API integration
â”‚       â”‚   â””â”€â”€ mistral_handler.py   # Mistral 7B integration
â”‚       â”œâ”€â”€ gui/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ main_window.py       # Main GUI window
â”‚       â”‚   â”œâ”€â”€ components.py        # GUI components
â”‚       â”‚   â””â”€â”€ dialogs.py           # Dialog windows
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ logger.py           # Logging system
â”‚           â””â”€â”€ helpers.py          # Utility functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reference_datasets/         # Downloaded Kaggle datasets
â”‚   â””â”€â”€ generated_datasets/         # Generated synthetic datasets
â”œâ”€â”€ logs/                           # Application logs
â”œâ”€â”€ tests/                          # Unit tests
â”œâ”€â”€ docs/                           # Documentation
â””â”€â”€ dist/                           # Distribution files

```
---
## ğŸ’» Requirements

```bash
pip install -r requirements.txt
```

---

## âš™ï¸ Setup

1. Set up your Kaggle API key (`kaggle.json`) in `~/.kaggle/`.
2. Install [Ollama](https://ollama.com/) and pull the Mistral model:

   ```bash
   ollama run mistral
   ```
3. Run the app:

   ```bash
   python main.py
   ```

---

## ğŸ“Œ Notes

* The synthetic datasets are **not exact replicas** but are **schema-aware variants** created by a locally run Mistral model.
* The dataset generation time varies based on:

  * Size of the original Kaggle dataset
  * Number of synthetic datasets requested
* API endpoints are **not yet implemented** â€” this will be included in the next release.

---

## ğŸ› ï¸ Planned Features (v2)

* âœ… Dataset download via API
* â³ REST API endpoints to access generated datasets
* âœ… Column selector and schema editor before generation
* âœ… Export logs and metadata with each dataset
* âœ… Lightweight CPU model support

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

[Vasishta Nandipati](https://github.com/Vasishta03)

---

## ğŸ™Œ Contributions

Feel free to open issues, suggest features, or fork the repo. All ideas are welcome!

